{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tmkin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "import json \n",
    "import requests\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_validation = pd.read_excel(\"validation_dataset.xlsx\")#.loc[0:10000,:]\n",
    "raw_train = pd.read_excel(\"training_dataset.xlsx\")#.loc[0:10000,:]\n",
    "raw_test = pd.read_excel(\"test_dataset.xlsx\")#.loc[0:10000,:]\n",
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text: split each string into words\n",
    "words = raw_train.loc[0:5000, 'text'].str.split().tolist()\n",
    "\n",
    "# Flatten the list of words\n",
    "all_words = list(itertools.chain.from_iterable(words))\n",
    "\n",
    "# Count the frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the most common words\n",
    "most_common_words = word_counts.most_common()  # You can specify a number inside most_common(n)\n",
    "\n",
    "# Display the most common words\n",
    "pprint(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"RemovedWords.json\", \"w+\") as file:\n",
    "    json.dump(list(np.array(most_common_words)[:,0]), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"FormattedWords.json\", \"r+\") as file:\n",
    "    most_common_words = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = raw_train.copy()\n",
    "validation_df = raw_validation.copy()\n",
    "test_df = raw_test.copy()\n",
    "\n",
    "\n",
    "# List of words to remove\n",
    "words_to_remove = list(np.array(most_common_words)[0:250,0])  # Add your words here\n",
    "\n",
    "# Create a regular expression pattern\n",
    "regex_pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words_to_remove) + r')\\b'\n",
    "\n",
    "# Remove the words from the DataFrame column\n",
    "train_df['text'] = raw_train['text'].str.strip().replace(regex_pattern, '', regex=True)\n",
    "test_df['text'] = raw_test['text'].str.strip().replace(regex_pattern, '', regex=True)\n",
    "validation_df['text'] = raw_validation['text'].str.strip().replace(regex_pattern, '', regex=True)\n",
    "print(\"Removed Common Words!\")\n",
    "\n",
    "# Replacing repeating words and one letter words\n",
    "test_df['text'] = test_df['text'].str.replace(r'\\b(\\w)(\\W*\\1)*\\b', '', regex=True)\n",
    "train_df['text'] = train_df['text'].str.replace(r'\\b(\\w)(\\W*\\1)*\\b', '', regex=True)\n",
    "validation_df['text'] = validation_df['text'].str.replace(r'\\b(\\w)(\\W*\\1)*\\b', '', regex=True)\n",
    "print(\"Removed Repeating words and one letter words\")\n",
    "\n",
    "# Remove sequences of spaces and all punctuation\n",
    "train_df['text'] = train_df['text'].str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n",
    "train_df['text'] = train_df['text'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove all punctuation except spaces and word characters\n",
    "\n",
    "test_df['text'] = test_df['text'].str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n",
    "test_df['text'] = test_df['text'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove all punctuation except spaces and word characters\n",
    "\n",
    "validation_df['text'] = validation_df['text'].str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n",
    "validation_df['text'] = validation_df['text'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove all punctuation except spaces and word characters\n",
    "\n",
    "train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Create instances of lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    lem_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lem_words)\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stem_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stem_words)\n",
    "\n",
    "train_df['stemmed_text'] = train_df['text'].progress_apply(lemmatize_sentence)\n",
    "train_df['lemmatized_text'] = train_df['text'].progress_apply(stem_sentence)\n",
    "\n",
    "test_df['stemmed_text'] = test_df['text'].progress_apply(lemmatize_sentence)\n",
    "test_df['lemmatized_text'] = test_df['text'].progress_apply(stem_sentence)\n",
    "\n",
    "validation_df['stemmed_text'] = validation_df['text'].progress_apply(lemmatize_sentence)\n",
    "validation_df['lemmatized_text'] = validation_df['text'].progress_apply(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"TrainData.csv\", index=False)\n",
    "test_df.to_csv(\"TestData.csv\", index=False)\n",
    "validation_df.to_csv(\"ValidationData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"TrainData.csv\")\n",
    "test_df = pd.read_csv(\"TestData.csv\")\n",
    "validation_df = pd.read_csv(\"ValidationData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['lemmatized_text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "x_validation = validation_df['lemmatized_text']\n",
    "y_validation = validation_df['label']\n",
    "\n",
    "x_test = test_df['lemmatized_text']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_validation = label_encoder.transform(y_validation)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features=1500)\n",
    "x_train = vectorizer.fit_transform(x_train.astype('U'))\n",
    "x_validation = vectorizer.transform(x_validation.astype('U'))\n",
    "x_test = vectorizer.transform(x_test.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train.toarray())\n",
    "x_validation = scaler.transform(x_validation.toarray())\n",
    "x_test = scaler.transform(x_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "kneighbors_model = KNeighborsClassifier(n_neighbors=10, weights='uniform')\n",
    "gaussiannb_model = GaussianNB()\n",
    "nn_model = MLPClassifier((), activation='logistic', verbose=1, batch_size=512, max_iter=50, warm_start=True)\n",
    "#xgboost_model = xgb.XGBClassifier(max_depth=10, n_estimators=100, learning_rate=0.1, colsample_bytree=0.5, subsample=0.5, n_jobs=-1, verbosity=1)\n",
    "decisiontree_model = DecisionTreeClassifier(max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82839996\n",
      "Iteration 2, loss = 1.45653795\n",
      "Iteration 3, loss = 1.41345701\n",
      "Iteration 4, loss = 1.39078836\n",
      "Iteration 5, loss = 1.37612235\n",
      "Iteration 6, loss = 1.36536211\n",
      "Iteration 7, loss = 1.35781288\n",
      "Iteration 8, loss = 1.35197974\n",
      "Iteration 9, loss = 1.34773770\n",
      "Iteration 10, loss = 1.34411368\n",
      "Iteration 11, loss = 1.34165982\n",
      "Iteration 12, loss = 1.33945112\n",
      "Iteration 13, loss = 1.33734788\n",
      "Iteration 14, loss = 1.33567875\n",
      "Iteration 15, loss = 1.33479536\n",
      "Iteration 16, loss = 1.33386873\n",
      "Iteration 17, loss = 1.33218840\n",
      "Iteration 18, loss = 1.33166002\n",
      "Iteration 19, loss = 1.33115716\n",
      "Iteration 20, loss = 1.33059619\n",
      "Iteration 21, loss = 1.32962825\n",
      "Iteration 22, loss = 1.32927377\n",
      "Iteration 23, loss = 1.32870703\n",
      "Iteration 24, loss = 1.32847379\n",
      "Iteration 25, loss = 1.32791327\n",
      "Iteration 26, loss = 1.32784429\n",
      "Iteration 27, loss = 1.32728447\n",
      "Iteration 28, loss = 1.32682559\n",
      "Iteration 29, loss = 1.32670234\n",
      "Iteration 30, loss = 1.32619317\n",
      "Iteration 31, loss = 1.32591213\n",
      "Iteration 32, loss = 1.32562040\n",
      "Iteration 33, loss = 1.32553095\n",
      "Iteration 34, loss = 1.32523432\n",
      "Iteration 35, loss = 1.32504364\n",
      "Iteration 36, loss = 1.32495902\n",
      "Iteration 37, loss = 1.32472336\n",
      "Iteration 38, loss = 1.32489572\n",
      "Iteration 39, loss = 1.32424797\n",
      "Iteration 40, loss = 1.32433320\n",
      "Iteration 41, loss = 1.32416958\n",
      "Iteration 42, loss = 1.32444388\n",
      "Iteration 43, loss = 1.32423525\n",
      "Iteration 44, loss = 1.32380615\n",
      "Iteration 45, loss = 1.32373644\n",
      "Iteration 46, loss = 1.32344045\n",
      "Iteration 47, loss = 1.32387025\n",
      "Iteration 48, loss = 1.32373696\n",
      "Iteration 49, loss = 1.32327479\n",
      "Iteration 50, loss = 1.32312356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tmkin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kneighbors_model.fit(x_train, y_train)\n",
    "gaussiannb_model.fit(x_train, y_train)\n",
    "nn_model.fit(x_train, y_train)\n",
    "#xgboost_model.fit(x_train, y_train)\n",
    "decisiontree_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neighbors Validation Accuracy: 18.445%\n",
      "Gaussian NB Validation Accuracy: 11.125%\n",
      "Neural Network Validation Accuracy: 54.480%\n",
      "Decision Tree Validation Accuracy: 32.957%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"K Neighbors Validation Accuracy: %.3f%%\" % (100 * accuracy_score(y_validation, kneighbors_model.predict(x_validation))))\n",
    "print(\"Gaussian NB Validation Accuracy: %.3f%%\" % (100 * accuracy_score(y_validation, gaussiannb_model.predict(x_validation))))\n",
    "print(\"Neural Network Validation Accuracy: %.3f%%\" % (100 * accuracy_score(y_validation, nn_model.predict(x_validation))))\n",
    "#print(\"XGBoost Validation Accuracy: %.3f%%\" % (100 * accuracy_score(y_validation, xgboost_model.predict(x_validation))))\n",
    "print(\"Decision Tree Validation Accuracy: %.3f%%\" % (100 * accuracy_score(y_validation, decisiontree_model.predict(x_validation))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47416/47416 [00:00<00:00, 160374.68it/s]\n",
      "100%|██████████| 47416/47416 [00:12<00:00, 3806.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Third  Second  First  Correct Label  In Top 3  Correct\n",
      "0     32      28     33             31     False    False\n",
      "1      7       2     52              2      True    False\n",
      "2     32      31     33             33      True     True\n",
      "3     13      14     46             46      True     True\n",
      "4     43      33     17             33      True    False\n",
      "Percent where Correct = First: 54.144%\n",
      "Percent where Correct in Top 3: 82.139%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "predictions = nn_model.predict_proba(x_test)\n",
    "predictions = np.argsort(predictions, axis=1)[:,-3:]\n",
    "class_labels = nn_model.classes_\n",
    "\n",
    "prediction_df = pd.DataFrame(predictions, columns=['Third', 'Second', 'First'])\n",
    "prediction_df['First'] = class_labels[prediction_df['First']]\n",
    "prediction_df['Second'] = class_labels[prediction_df['Second']]\n",
    "prediction_df['Third'] = class_labels[prediction_df['Third']]\n",
    "prediction_df['Correct Label'] = y_test\n",
    "prediction_df['In Top 3'] = False\n",
    "\n",
    "prediction_df['Correct'] = prediction_df.progress_apply(lambda row: True if row['Correct Label'] == row['First'] else False, axis=1)\n",
    "prediction_df['In Top 3'] = prediction_df.progress_apply(lambda row: row['Correct Label'] in row[['First', 'Second', 'Third']].values, axis=1)\n",
    "\n",
    "print(prediction_df.head())\n",
    "\n",
    "# find percent where Correct = First\n",
    "print(\"Percent where Correct = First: %.3f%%\" % (100 * len(prediction_df[prediction_df['Correct'] == True])/len(prediction_df)))\n",
    "print(\"Percent where Correct in Top 3: %.3f%%\" % (100 * len(prediction_df[prediction_df['In Top 3'] == True])/len(prediction_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save neural net model to .pkl\n",
    "import pickle\n",
    "pickle.dump(nn_model, open(\"nn_model.pkl\", \"wb\"))\n",
    "pd.DataFrame(nn_model.predict_proba(x_test)).to_csv(\"prob.csv\", index=False)\n",
    "pd.DataFrame(y_test).to_csv(\"y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
